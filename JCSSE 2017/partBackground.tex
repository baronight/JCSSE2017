
เนื่องจากในการประมวลผลข้อความต่าง ๆ เราจำเป็นต้องทราบถึงคำแต่ละคำในข้อความนั้น เพื่อที่เราจะสามารถนำคำเหล่านั้นมาพิจารณาได้ จึงทำให้เกิดกระบวนการการตัดคำขึ้นมา แต่ทั้งนี้ทั้งนั้นการตัดคำเพียงอย่างเดียว อาจจะไม่เพียงพอต่อการนำคำต่าง ๆ ที่ได้ไปประมวลผล เนื่องจากคำบางคำมีความหมายและหน้าที่ของคำที่ต่างกัน จึงเป็นเหตุให้นอกจากการตัดคำเพียงอย่างเดียวนั้นไม่สามารถแยกแยะความหมายและหน้าที่ของคำได้ ทำให้ต้องมีกระบวนการหาหน้าที่ของคำเหล่านั้นขึ้นมาด้วย

ซึ่งในงานวิจัยนี้มีความต้องการที่จะวิเคราะห์หาหัวข้อที่แสดงถึงทัศนคติของความคิดเห็นของผู้ใช้งานโปรแกรมบนอุปกรณ์ smart device ทำให้งานวิจัยนี้เกี่ยวข้องกับการประมวลผลข้อความ ซึ่งต้องมีการตัดคำและหาหน้าที่ของคำเหล่านั้น และนอกจากนั้นงานวิจัยนี้ยังต้องหาหัวข้อของความคิดเห็น และวิเคราะห์ทัศนคติของความคิดเห็นเหล่านั้นด้วย

ดังนั้นงานวิจัยนี้จึงมีทฤษฎีที่เกี่ยวข้องอยู่ 4 ส่วนคือ การตัดคำ การหาหน้าที่ของคำ การหาหัวข้อของข้อความต่าง ๆ และการวิเคราะห์ทัศนคติของประโยค
\subsection{Word Segmentation}
การตัดคำในประโยคภาษาอังกฤษนั้นเราสามารถทำได้โดยไม่ยากเนื่องจากเราจะใช้ช่องว่างในการแบ่งคำหรือใช้ '.' ในการจบประโยค หรือใช้ '?' ในการจบประโยคสำหรับคำถาม แต่สำหรับประโยคภาษาไทยนั้นจะมีลักษณะที่คล้ายกับภาษาจีนและญี่ปุ่นตรงที่เราไม่มีการเว้นวรรคคำแต่ละคำในประโยค ทำให้การหาคำภาษาไทยมีความลำบากมากกว่าภาษาอังกฤษ\cite{featurethaiwordseg}

โดยแนวคิดสำหรับการตัดคำในปัจจุบันนั้นมีหลายแนวคิด เช่น
\subsubsection{Longest Matching\cite{syllableseparator}}
วิธีการนี้เป็นการนำสายอักขระทั้งสายมาเปรียบเทียบกับคำที่อยู่ในพจนานุกรม (lexicon) โดยถ้ามีคำตรงกับใน lexicon วิธีนี้ก็จะนำคำที่ได้ออกมาสายอักขระ แต่ถ้าเทียบแล้วไม่พบใน lexicon วิธีนี้จะใช้วิธีตัดตัวอักษรตัวสุดท้ายของสายอักขระออกไป 1 ตัวอักษร แล้วทำการเทียบกับ lexicon ใหม่ โดยจะทำอย่างนี้จนจบทั้งสายอักขระ

ตัวอย่างคำที่ได้จากวิธีนี้ เช่น "ฉันนั่งตากลมอยู่ริมตลิ่ง" จะได้ "ฉัน นั่ง ตาก ลม อยู่ ริม ตลิ่ง"
\subsubsection{Maximal Matching\cite{wordsegforthai}}
วิธีการนี้เป็นการทดลองตัดคำที่มีโอกาสเกิดขึ้นได้ทุกรูปแบบก่อน จากนั้นจะทำการเลือกรูปแบบที่มีจำนวนคำที่น้อยที่สุดออกมา แต่ถ้าเกิดรูปแบบที่มีคำน้อยที่สุดมีหลายรูปแบบ วิธีการนี้ก็จะนำวิธีการ Longest Matching เข้ามาช่วยในการตัดสินใจ

ตัวอย่างคำที่ได้จากวิธีการนี้ เช่น "ไปหามเหสี" ซึ่งมีโอกาสได้คำว่า "ไป หาม เห สี" และ "ไป หา มเหสี" โดยวิธีการนี้จะเลือกคำว่า "ไป หา มเหสี" ออกมา
\subsubsection{Probabilistic Model\cite{thaiwordfilter}}
วิธีการนี้จะมีการใช้ข้อมูลเชิงสถิติการเกิดคำและหน้าที่ของคำ เข้ามาช่วยในการหาโอกาสของคำที่จะเกิดขึ้นในประโยคที่มากที่สุด ซึ่งวิธีการนี้จำเป็นต้องมี corpus ที่มีข้อมูลของคำและหน้าที่ของคำที่ถูกต้อง เพื่อนำมาใช้ในการคำนวณสถิติของคำที่จะเกิดขึ้น
\subsubsection{Feature-based Approach\cite{featurethaiwordseg}}

วิธีการนี้จะมีการพิจารณาจากบริบทและการเกิดร่วมกันของคำ มาตัดสินใจในการตัดคำนั้น ๆ

เช่น คำว่า "มากว่า" ถ้าคำที่มาต่อท้ายคำนี้เป็นตัวเลข วิธีนี้จะตัดสินใจว่าควรจะได้คำว่า "มา กว่า"

%end Feature-based Approach
ซึ่งในงานวิจัยนี้ผู้วิจัยเลือกใช้วิธีการตัดคำแบบ Longest Matching โดยใช้โปรแกรม LexTo\cite{LexTo} ซึ่งเป็นโปรแกรมที่ถูกพัฒนาโดย National Electronics and Computer Technology Center (NECTEC) ซึ่งรองรับในการตัดคำด้วยวิธีการ Longest Matching
\subsection{Part of Speech}
หน้าที่ของคำเป็นสิ่งที่ใช้กำหนดชนิดของคำในข้อความนั้น ๆ โดยชนิดของคำสามารถแบ่งได้ 8 ประเภทใหญ่ คือ
1. noun,
2. pronoun,
3. verb,
4. adverb,
5. adjective,
6. preposition,
7. conjunction,
8. interjection

โดยการหาหน้าที่ของคำนั้นเป็นส่วนหนึ่งในการประมวลผลภาษาธรรมชาติ ซึ่งจำเป็นต้องมี corpus สำหรับการเรียนรู้หน้าที่ของคำ เพื่อที่เราจะสามรถตอบได้ว่าคำที่มีอยู่ในข้อความนั้นเป็นคำชนิดอะไร และ corpus ของภาษาไทยที่สามารถพบเห็นได้บ่อยคือ NAiST\cite{NAiST} corpus ซึ่งเป็นคลังข้อมูลที่พัฒนาโดยคณะวิศกรรมคอมพิวเตอร์ มหาวิทยาลัยเกษตรศาสตร์ และ ORCHID\cite{ORCHID} corpus ที่พัฒนาโดย NECTEC

ในการหาหน้าที่ของคำนั้นมีเครื่องมือที่ช่วยในการค้นหาอยู่มากมายตัวอย่างเช่น Nature Language ToolKit (NLTK)\cite{NLTK} เป็นเครื่องมือที่ใช้ในการประมวลภาษาธรรมชาติภาษาอังกฤษ ซึ่งรองรับการเพิ่ม corpus อื่นๆ นอกจากที่มีอยู่ในระบบอยู่แล้ว 
และ RDRPOStagger\cite{RDRPOSTagger} เป็นเครื่องมือสำหรับการหาหน้าที่ของคำโดยเฉพาะ โดยมี corpus 7 ภาษา รวมถึงภาษาไทย (ซึ่งใช้ orchid เป็น corpus)

\subsection{Topic Modeling}%not finish
Topic Modeling เป็นวิธีการจัดกลุ่มหัวข้อของประโยคที่เรากำลังพิจารณาอยู่ โดยการคาดเดาความน่าจะเป็นของคำที่จะเกิดในกลุ่มของหัวข้อนั้น ๆ ซึ่งจะมีวิธีการหาหัวข้อได้ 2 แบบหลัก ๆ คือ
\subsubsection{Aspect and Sentiment Unification Model (ASUM)\cite{asum}}
เป็นวิธีการหาหัวข้อของประโยค โดยมีหลักการว่า 1 ประโยค จะมีผู้กล่าวจะกล่าวถึงหัวข้อเพียงหัวข้อเดียว
\subsubsection{Latent Dirichlet Allocation (LDA)\cite{LDA}}
มีแนวคิดที่ว่าในประโยค อาจจะมีหัวข้อที่ถูกกล่าวถึงมากกว่า 1 หัวข้อ โดยในแต่ละหัวข้อจะเกิดจากการรวมกันของคำหลาย ๆ คำ ซึ่งแต่ละคำในหัวข้อก็จะมีความน่าจะเป็นที่แตกต่างกัน

\subsection{Sentiment Analysis}
Sentiment Analysis หรือ Opinion mining เป็นการศึกษาเกี่ยวกับความรู้สึก อารมณ์ ทัศนคติ จากการสังเกตเนื้อหาของการสนทนาเหล่านั้น\cite{surveyopinionmining} โดยในการทำ sentiment analysis นั้นมีวิธีการวิเคราะห์พื้นฐาน 2 แบบคือ
\subsubsection{lexicon-based}
คือการนำคำที่ต้องการหาค่าทัศนคติไปเปรียบเทียบกับ lexicon ที่มีค่าทัศนคติกำกับอยู่ เพื่อใช้เป็นตัวแทนของค่าทัศนคติของคำนั้น ๆ
\subsubsection{machine learning-based}
คือ การนำข้อมูลที่มีค่าทัศนคติมาฝึกฝนการวิเคราะห์ของคอมพิวเตอร์ เพื่อนำไปเป็นฐานสำหรับการวิเคราะห์ข้อมูลอื่น ๆ ต่อไป
